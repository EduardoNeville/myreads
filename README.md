# MoE-Mamba Papers and Notes

![MoE-Mamba](assets/MoEMamba.png)

Find my notes in the [markdown](notes.md) file

## Links to the papers

- [MoE-Mamba](https://arxiv.org/pdf/2401.04081.pdf)

## MoE (Mixture of Experts) Papers

- [Mixtral-of-Experts](https://arxiv.org/abs/2401.04088) 
- [ScalableandEfficientMoETrainingforMultitaskMultilingualModels](https://arxiv.org/abs/2109.10465)
- [ST-MoE](https://arxiv.org/abs/2202.08906)
- [UNIFIED-SCALING-LAWS-FOR-ROUTED-LANGUAGE-MODELS](https://arxiv.org/abs/2202.01169)
- [Switch-Transformers](https://arxiv.org/abs/2101.03961)
- [LimitsofTransferLearningwhUnified](https://arxiv.org/abs/1910.10683)
- [EMPIRICAL-UNDERSTANDING-OF-MOE-DESIGN-CHOICES](https://arxiv.org/abs/2402.13089)
- [OUTRAGEOUSLY-LARGE-NEURAL-NETWORKS](https://arxiv.org/abs/1701.06538)
- [MoE-CROSS-EXAMPLE-AGGREGATION](https://arxiv.org/abs/2310.15961)
- [Transferable-Adversarial-Robustness-for-Categorical-Data-via-Universal-Robust-Embedding](https://arxiv.org/abs/2306.04064)
- [Hash-Layers-For-Large-Sparse-Models](https://arxiv.org/abs/2106.04426)

## Mamba

- [How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections](https://arxiv.org/abs/2206.12037v2)
- [Combining-Recurrent-Convolutional-and-Continuous-time-Models-with-Linear-State-Space-Layers](https://arxiv.org/abs/2110.13985)
- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)
- [Vision-Mamba](https://arxiv.org/abs/2401.09417)
- [RepeatAfterMe](https://arxiv.org/abs/2402.01032)
- [Efficiently-ModLongSeqwhSSS](https://arxiv.org/abs/2111.00396)
- [Hungry Hungry Hippos: Towards Language Modeling with State Space Models](https://arxiv.org/abs/2212.14052)
- [MambaByte](https://arxiv.org/abs/2401.13660)
- [Legendre Memory Units](http://papers.nips.cc/paper/9689-legendre-memory-units-continuous-time-representation-in-recurrent-neural-networks)
- [HiPPO Recurrent Memory](https://arxiv.org/abs/2008.07669)



